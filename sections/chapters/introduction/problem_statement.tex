\subsection{Context}
Companies often have to deal with large amount of unstructured data.
This data can come from many different sources (such as sensors, systems running 24/7, and users) in a variety of
different formats.
As technology evolves, the rate at which data is gathered only ever increases, as computer systems gain new ways to
extract information from the environment, and people leave a digital footprint in almost every daily activity.

\subsection{Issue}
The problem with this development is that as a data set grows, it often loses its meaning.
Data is seldom stored in a perfect manner, with annotations explaining what each entry represents and what can we
expect from a file with millions of rows.
The sizes of these files often make it impossible to manually analyze them and determine the nature of the data stored within.
When data analysts take these files and input them into a system like Microsoft Power BI, they produce graphs that often fail to
provide any real insights. 
It is pointless to look at numbers in a diagram if we don't know what these numbers mean.

\subsection{Objectives}
The purpose of this project is to develop a tool that can extract valuable insights out of a large amount of unstructured data.
Essentially, the final product should be used by a customer before inputting the data into a data visualization tool like Power BI.
The additional information (metadata) produced by our system should be enough for an analyst to determine the kind of data
he is dealing with: whether it was produced by sensors or humans, the period time it was extracted in, and so on.
We will be using similarity techniques, a formal representation of metadata and labelling approaches to determine, as
accurately as possible, what kind of data we are presented with in each scenario.